{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/david/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/david/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import uuid\n",
    "import time\n",
    "\n",
    "from joblib import dump, load\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "nltk.download('punkt') ## algorithm for tokenization\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "unreliable_weight = 0.5\n",
    "train['weight'] = train['label_quality'].apply(lambda quality: 1. if quality == 'reliable' else unreliable_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spanish = train[train.language == 'spanish']\n",
    "train_portuguese = train[train.language == 'portuguese']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spanish_reliable = train_spanish[train_spanish.label_quality == 'reliable']\n",
    "train_portuguese_reliable = train_portuguese[train_portuguese.label_quality == 'reliable']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_small_categories(df):\n",
    "    grouped = df.groupby(['category']).count()\n",
    "    available_categories = list(grouped[grouped['title'] > 1].index.to_numpy())\n",
    "    filtered = df[df.category.isin(available_categories)]\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_preprocessor(doc):\n",
    "    return re.sub('[0-9¡¨ª®°´·º»½¿ø' + string.punctuation + ']', '', doc).lower()\n",
    "\n",
    "spanish_stemmer = SnowballStemmer('spanish')\n",
    "portuguese_stemmer = SnowballStemmer('portuguese')\n",
    "\n",
    "def spanish_stemmer_tokenizer(doc):\n",
    "    tokens = word_tokenize(doc)\n",
    "    return [spanish_stemmer.stem(token) for token in tokens]\n",
    "\n",
    "def portuguese_stemmer_tokenizer(doc):\n",
    "    tokens = word_tokenize(doc)\n",
    "    return [portuguese_stemmer.stem(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elapsed_time(start, message):\n",
    "    end = time.time()\n",
    "    print( message + ': ', end-start )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(titles, labels, weights, language='spanish', batches=200, epochs=400, max_features=8000, min_df=3, max_df=0.7):\n",
    "    titles_train, titles_test, y_train, y_test, w_train, w_test = train_test_split(titles, labels, weights, test_size=0.1, random_state=42, stratify=labels)\n",
    "    print(titles_train.shape, titles_test.shape, y_train.shape, y_test.shape)\n",
    "    \n",
    "    run_id = uuid.uuid4().hex\n",
    "    print('Run id: ', run_id)\n",
    "    \n",
    "    if language == 'spanish':\n",
    "        stop_words = list(map(lambda word: spanish_stemmer.stem(word), stopwords.words(language)))\n",
    "        vectorizer = CountVectorizer(max_features=max_features, max_df=max_df, min_df=min_df, strip_accents='unicode', \n",
    "                                        stop_words=stop_words, tokenizer=spanish_stemmer_tokenizer, \n",
    "                                        preprocessor=my_preprocessor)\n",
    "    else:\n",
    "        stop_words = list(map(lambda word: portuguese_stemmer.stem(word), stopwords.words(language)))\n",
    "        vectorizer = CountVectorizer(max_features=max_features, max_df=max_df, min_df=min_df, strip_accents='unicode', \n",
    "                                        stop_words=stop_words, tokenizer=portuguese_stemmer_tokenizer, \n",
    "                                        preprocessor=my_preprocessor)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    vectorizer.fit(titles_train)\n",
    "    elapsed_time(start_time, 'Fit vectorizer')\n",
    "    \n",
    "    dump(vectorizer, 'models/' + language + '_vectorizer_' + run_id + '.joblib')\n",
    "    \n",
    "    tokens = vectorizer.get_feature_names()\n",
    "\n",
    "    start_time = time.time()\n",
    "    X_train = vectorizer.transform(titles_train)\n",
    "    elapsed_time(start_time, 'Word2Vec X_train')\n",
    "    \n",
    "    start_time = time.time()\n",
    "    X_test = vectorizer.transform(titles_test)\n",
    "    elapsed_time(start_time, 'Word2Vec X_test')\n",
    "    \n",
    "    samples = X_train.shape[0]\n",
    "    print('samples per batch: ', samples//batches)\n",
    "\n",
    "    classifier = MultinomialNB()\n",
    "    \n",
    "    categories = y_train.unique()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        for batch in range(batches):\n",
    "            start_time = time.time()\n",
    "            start = (samples * batch)//batches\n",
    "            end = (samples * (batch + 1))//batches\n",
    "            classifier.partial_fit(X_train[ start:end ], y_train[ start:end ], \n",
    "                                   sample_weight=w_train[ start:end ], classes=categories)\n",
    "            if(batch==1):\n",
    "                elapsed_time(start_time, 'Batch')\n",
    "        if epoch % 5 == 0:\n",
    "            y_predicted = classifier.predict(X_test)\n",
    "            score = balanced_accuracy_score(y_true=y_test, y_pred=y_predicted, sample_weight=w_test)\n",
    "            print('finished epoch: ', epoch, 'with score:', score)\n",
    "            dump(classifier, 'models/checkpoint/' + language + 'classifier_' + run_id + '_' + str(epoch) + '.joblib') \n",
    "\n",
    "    y_predicted = classifier.predict(X_test)\n",
    "    score = balanced_accuracy_score(y_true=y_test, y_pred=y_predicted)\n",
    "    print('Final score: ', score)\n",
    "\n",
    "    dump(classifier, 'models/' + language + '_classifier_' + run_id + '.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portuguese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered = filter_small_categories(train_portuguese_reliable)\n",
    "# train(filtered['title'], filtered['category'], language='portuguese')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtering categories:  3.737879514694214\n",
      "(9000000,) (1000000,) (9000000,) (1000000,)\n",
      "Run id:  439c8b8b558f49f79e5dd1b5c2e467b2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/miniconda3/envs/ml-challenge/lib/python3.5/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['tambi'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit vectorizer:  2024.0291199684143\n",
      "Word2Vec X_train:  2057.8113403320312\n",
      "Word2Vec X_test:  238.27207207679749\n",
      "samples per batch:  45000\n",
      "Batch:  4.319194793701172\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "filtered = filter_small_categories(train_spanish)\n",
    "elapsed_time(start_time, 'filtering categories')\n",
    "\n",
    "train(filtered['title'], filtered['category'], filtered['weight'], language='spanish', max_features = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
